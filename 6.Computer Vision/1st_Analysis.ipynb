{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 : Augmentation외 다른 전처리는 하지 않았습니다.\n",
    "\n",
    "학습 :\n",
    "\n",
    "1) EfficientNetB8 (imagenet + adversarial training pre-trained weight 사용)  \n",
    "2) 5fold  \n",
    "3) Augmentation : 90~360 회전, Cutout + Non-rigid transform 몇 개  \n",
    "4) Adam, MultiLabelSoftMarginLoss, ReduceLROnPlateau Scheduler, Dropout=50  \n",
    "\n",
    "후처리 : TTA (90~360회전), soft voting, prediction score > 0.35 이면 해당 class를 1로 라벨링\n",
    "\n",
    "LB : 리더보드 TTA: Test Time Augmentation\n",
    "단일모델 -> LB 0.8774 (thres 0.5)  \n",
    "단일모델 + TTA -> LB 0.8841 (thres 0.5)  \n",
    "5fold(hard vote) + TTA -> LB 0.8889 (thres 0.5)  \n",
    "5fold(soft vote) + TTA -> LB 0.8901 (thres 0.5)  \n",
    "5fold(soft vote) + TTA -> LB 0.8902 (thres 0.35) 입니다.  \n",
    "\n",
    "여러가지 테스트 한 결과 참고 바랍니다.\n",
    "\n",
    "1) Augmetnation : 코드에 있는 옵션 외 다른 옵션을 더 추가할 때 마다 validation loss는 줄지만, test score는 더 안 좋아졌습니다.  \n",
    "2) 이미지 전처리 : 선형, 저주파 통과, bilateral 필터 등등 이것저것 해봤는데 성능이 더 안 좋아졌습니다. 눈으로 보기에는 bilateral 필터가 가장 효과가 좋았습니다.  \n",
    "3) 이전 대회 데이터 : 이미지넷 + 이전 대회 이미지 학습 (크기도, 위치도 랜덤) 후, 대회 데이터 학습 하였는데 성능이 더 안 좋아졌습니다.  \n",
    "4) gradient clipping : 코드에는 들어가 있지만, 큰 효과는 없는 것 같습니다.  \n",
    "5) TTA : 회전 외 다른 옵션을 추가할 때마다 성능이 낮아졌습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient Net 이란?\n",
    "====\n",
    "\n",
    "Neural Architecture Search(강화학습 기반의 network 탐색 기법)\n",
    "\n",
    "1. Depth (d) (layer를 깊게 가져가는것, ex. ResNet-100 -> ResNet-1000)  \n",
    "\n",
    "2. Width (w) (# of channels를 많게 가져가는것)  \n",
    "\n",
    "3. Resolution (r) (input image의 사이즈를 MxM -> r*Mxr*M의, 더 큰사이즈로 입력받는것) \n",
    "\n",
    "=>위 세가지 요소를 Compund Scaling 하여 적절한 파라미터 설정\n",
    "\n",
    "![파라미터 대비 성능](Efficient1.png)\n",
    "\n",
    "![피쳐 추출 결과 예시](Parameter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import easydict\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import tqdm\n",
    "import cv2\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch\n",
    "import albumentations\n",
    "import albumentations.pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Albumentaion Document**  \n",
    "https://albumentations.ai/docs/  \n",
    "\n",
    "**Albumentation GitHub**  \n",
    "https://github.com/albumentations-team/albumentations  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"image_path\": \"./data/dirty_mnist_2nd/\",\n",
    "    \"label_path\": \"./data/dirty_mnist_2nd_answer.csv\",\n",
    "    \"kfold_idx\": 0, ## 0~4까지 바꿔 가면서 5번 학습합니다. \n",
    "\n",
    "    \"model\": \"efficientnet-b8\",\n",
    "    \"epochs\": 2000,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 1e-3,\n",
    "    \"patience\": 8,\n",
    "\n",
    "    \"resume\": None,\n",
    "    \"device\": device,\n",
    "    \"comments\": None,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class DatasetMNIST(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, label_df, transforms):        \n",
    "        self.image_folder = image_folder   \n",
    "        self.label_df = label_df\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_df)\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        image_fn = self.image_folder +\\\n",
    "            str(self.label_df.iloc[index,0]).zfill(5) + '.png'\n",
    "                                              \n",
    "        image = cv2.imread(image_fn, cv2.IMREAD_GRAYSCALE)        \n",
    "        image = image.reshape([256, 256, 1])\n",
    "\n",
    "        label = self.label_df.iloc[index,1:].values.astype('float')\n",
    "\n",
    "        if self.transforms:            \n",
    "            image = self.transforms(image=image)['image'] / 255.0 #스케일링\n",
    "\n",
    "        return image, label\n",
    "\n",
    "mnist_transforms = {\n",
    "    'train' : albumentations.Compose([ \n",
    "            albumentations.RandomRotate90(),\n",
    "            albumentations.OneOf([ #다음 augmentation \n",
    "                albumentations.GridDistortion(distort_limit=(-0.3, 0.3), border_mode=cv2.BORDER_CONSTANT, p=1), #그리드 왜곡\n",
    "                albumentations.ShiftScaleRotate(rotate_limit=15, border_mode=cv2.BORDER_CONSTANT, p=1), #Scale 및 Rotate       \n",
    "                albumentations.ElasticTransform(alpha_affine=10, border_mode=cv2.BORDER_CONSTANT, p=1), #그리드 왜곡\n",
    "            ], p=1),    \n",
    "            albumentations.Cutout(num_holes=16, max_h_size=15, max_w_size=15, fill_value=0), #성겅성겅\n",
    "            albumentations.pytorch.ToTensorV2(),\n",
    "        ]),\n",
    "    'valid' : albumentations.Compose([        \n",
    "        albumentations.pytorch.ToTensorV2(),\n",
    "        ]),\n",
    "    'test' : albumentations.Compose([        \n",
    "        albumentations.pytorch.ToTensorV2(),\n",
    "        ]),\n",
    "}\n",
    "\n",
    "def train(train_loader, model, loss_func, device, optimizer, scheduler=None):\n",
    "    n = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    model.train()    \n",
    "\n",
    "    with tqdm.tqdm(train_loader, total=len(train_loader), desc=\"Train\", file=sys.stdout) as iterator:\n",
    "        for train_x, train_y in iterator:\n",
    "            train_x = train_x.float().to(device)\n",
    "            train_y = train_y.float().to(device)\n",
    "            \n",
    "            output = model(train_x)\n",
    "            \n",
    "            loss = loss_func(output, train_y)\n",
    "            \n",
    "            n += train_x.size(0)\n",
    "            running_loss += loss.item() * train_x.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / float(n)\n",
    "\n",
    "            output = output > 0.5\n",
    "            running_corrects += (output == train_y).sum()\n",
    "            epoch_acc = running_corrects / train_y.size(1) / n\n",
    "\n",
    "            log = 'loss - {:.5f}, acc - {:.5f}'.format(epoch_loss, epoch_acc)\n",
    "            \n",
    "            iterator.set_postfix_str(log)\n",
    "\n",
    "            optimizer.zero_grad() #역전파 실행전 grad 0으로 초기화역전파 실행전 grad 0으로 초기화\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1) #gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(valid_loader, model, loss_func, device, scheduler=None):\n",
    "    n = 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with tqdm.tqdm(valid_loader, total=len(valid_loader), desc=\"Valid\", file=sys.stdout) as iterator: #tqdm 상태바\n",
    "        for train_x, train_y in iterator:\n",
    "            train_x = train_x.float().to(device)\n",
    "            train_y = train_y.float().to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(train_x)\n",
    "            \n",
    "            loss = loss_func(output, train_y)\n",
    "\n",
    "            n += train_x.size(0)\n",
    "            running_loss += loss.item() * train_x.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / float(n)\n",
    "\n",
    "            output = output > 0.5\n",
    "            running_corrects += (output == train_y).sum()\n",
    "            epoch_acc = running_corrects / train_y.size(1) / n\n",
    "\n",
    "            log = 'loss - {:.5f}, acc - {:.5f}'.format(epoch_loss, epoch_acc)\n",
    "\n",
    "            iterator.set_postfix_str(log)\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 50)\n",
    "print('[info msg] arguments\\n')\n",
    "for key, value in vars(args).items():\n",
    "    print(key, \":\", value)\n",
    "print('=' * 50)\n",
    "\n",
    "assert os.path.isdir(args.image_path), 'wrong path'\n",
    "assert os.path.isfile(args.label_path), 'wrong path'\n",
    "if (args.resume):\n",
    "    assert os.path.isfile(args.resume), 'wrong path'\n",
    "assert args.kfold_idx < 5\n",
    "\n",
    "seed_everything(777)\n",
    "\n",
    "data_set = pd.read_csv(args.label_path)\n",
    "valid_idx_nb = int(len(data_set) * (1 / 5))\n",
    "valid_idx = np.arange(valid_idx_nb * args.kfold_idx, valid_idx_nb * (args.kfold_idx + 1))\n",
    "\n",
    "print('[info msg] validation fold idx !!\\n')        \n",
    "print(valid_idx)\n",
    "print('=' * 50)\n",
    "\n",
    "train_data = data_set.drop(valid_idx)\n",
    "valid_data = data_set.iloc[valid_idx]\n",
    "\n",
    "train_set = DatasetMNIST(\n",
    "    image_folder=args.image_path,\n",
    "    label_df=train_data,\n",
    "    transforms=mnist_transforms['train']\n",
    ")\n",
    "\n",
    "valid_set = DatasetMNIST(\n",
    "    image_folder=args.image_path,\n",
    "    label_df=valid_data,\n",
    "    transforms=mnist_transforms['valid']\n",
    ")\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "model = None\n",
    "\n",
    "if(args.resume):\n",
    "    model = EfficientNet.from_name(args.model, in_channels=1, num_classes=26, dropout_rate=0.5)\n",
    "    model.load_state_dict(torch.load(args.resume))\n",
    "    print('[info msg] pre-trained weight is loaded !!\\n')        \n",
    "    print(args.resume)\n",
    "    print('=' * 50)\n",
    "\n",
    "else:\n",
    "    print('[info msg] {} model is created\\n'.format(args.model))\n",
    "    model = EfficientNet.from_pretrained(args.model, in_channels=1, num_classes=26, dropout_rate=0.5, advprop=True)\n",
    "    print('=' * 50)\n",
    "\n",
    "if args.device == 'cuda' and torch.cuda.device_count() > 1 :\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), args.lr)\n",
    "criterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer,\n",
    "    mode='min',\n",
    "    patience=2,\n",
    "    factor=0.5,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "valid_loss = []\n",
    "valid_acc = []\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "patience = 0\n",
    "\n",
    "date_time = datetime.now().strftime(\"%m%d%H%M%S\")\n",
    "SAVE_DIR = os.path.join('./save', date_time)\n",
    "\n",
    "print('[info msg] training start !!\\n')\n",
    "startTime = datetime.now()\n",
    "for epoch in range(args.epochs):        \n",
    "    print('Epoch {}/{}'.format(epoch+1, args.epochs))\n",
    "    train_epoch_loss, train_epoch_acc = train(\n",
    "        train_loader=train_data_loader,\n",
    "        model=model,\n",
    "        loss_func=criterion,\n",
    "        device=args.device,\n",
    "        optimizer=optimizer,\n",
    "        )\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    train_acc.append(train_epoch_acc)\n",
    "\n",
    "    valid_epoch_loss, valid_epoch_acc = validate(\n",
    "        valid_loader=valid_data_loader,\n",
    "        model=model,\n",
    "        loss_func=criterion,\n",
    "        device=args.device,\n",
    "        scheduler=scheduler,\n",
    "        )\n",
    "    valid_loss.append(valid_epoch_loss)        \n",
    "    valid_acc.append(valid_epoch_acc)\n",
    "\n",
    "    if best_loss > valid_epoch_loss:\n",
    "        patience = 0\n",
    "        best_loss = valid_epoch_loss\n",
    "\n",
    "        Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'model_best.pth.tar'))\n",
    "        print('MODEL IS SAVED TO {}!!!'.format(date_time))\n",
    "        \n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience > args.patience - 1:\n",
    "            print('=======' * 10)\n",
    "            print(\"[Info message] Early stopper is activated\")\n",
    "            break\n",
    "\n",
    "elapsed_time = datetime.now() - startTime\n",
    "\n",
    "train_loss = np.array(train_loss)\n",
    "train_acc = np.array(train_acc)\n",
    "valid_loss = np.array(valid_loss)\n",
    "valid_acc = np.array(valid_acc)\n",
    "\n",
    "best_loss_pos = np.argmin(valid_loss)\n",
    "\n",
    "print('=' * 50)\n",
    "print('[info msg] training is done\\n')\n",
    "print(\"Time taken: {}\".format(elapsed_time))\n",
    "print(\"best loss is {} w/ acc {} at epoch : {}\".format(best_loss, valid_acc[best_loss_pos], best_loss_pos))    \n",
    "\n",
    "print('=' * 50)\n",
    "print('[info msg] {} model weight and log is save to {}\\n'.format(args.model, SAVE_DIR))\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, 'log.txt'), 'w') as f:\n",
    "    for key, value in vars(args).items():\n",
    "        f.write('{} : {}\\n'.format(key, value))            \n",
    "\n",
    "    f.write('\\n')\n",
    "    f.write('total ecpochs : {}\\n'.format(str(train_loss.shape[0])))\n",
    "    f.write('time taken : {}\\n'.format(str(elapsed_time)))\n",
    "    f.write('best_train_loss {} w/ acc {} at epoch : {}\\n'.format(np.min(train_loss), train_acc[np.argmin(train_loss)], np.argmin(train_loss)))\n",
    "    f.write('best_valid_loss {} w/ acc {} at epoch : {}\\n'.format(np.min(valid_loss), valid_acc[np.argmin(valid_loss)], np.argmin(valid_loss)))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(valid_loss, 'o', label='valid loss')\n",
    "plt.axvline(x=best_loss_pos, color='r', linestyle='--', linewidth=1.5)\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc, label='train acc')\n",
    "plt.plot(valid_acc, 'o', label='valid acc')\n",
    "plt.axvline(x=best_loss_pos, color='r', linestyle='--', linewidth=1.5)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(SAVE_DIR, 'history.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
