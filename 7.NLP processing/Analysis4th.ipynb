{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 목차\n",
    "====\n",
    "**데이터 전처리**  \n",
    "\n",
    "1. MultinomialNB을 이용한 feature 생성  \n",
    "\n",
    "2. CNN을 이용한 feature 생성  \n",
    "\n",
    "3. GRU를 이용한 feature 생성  \n",
    "\n",
    "4. NN을 이용한 feature 생성  \n",
    "\n",
    "5. 최종 stacking ensemble  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글 드라이브에 마운트합니다.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#경로 설정\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks/소설작가분류AI경진대회')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test_x.csv\")\n",
    "\n",
    "# 단어수(중복 포함)\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# 단어수(중복 제거)\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# 글자수\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "# stopwords : nltk의 stopwords보다 월등한 성능을 보여줍니다\n",
    "stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n",
    "\n",
    "# punctuation의 개수\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "# 대문자로만 이루어진 단어 개수\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "# 첫글자가 대문자인 단어 개수\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "# text 평균 길이\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>39</td>\n",
       "      <td>240</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.239130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>50</td>\n",
       "      <td>320</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.614035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>49</td>\n",
       "      <td>319</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.517241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>228</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.871795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  author  \\\n",
       "0      0  He was almost choking. There was so much, so m...       3   \n",
       "1      1             “Your sister asked for it, I suppose?”       2   \n",
       "2      2   She was engaged one day as she walked, in per...       1   \n",
       "3      3  The captain was in the porch, keeping himself ...       4   \n",
       "4      4  “Have mercy, gentlemen!” odin flung up his han...       3   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         46                39        240             27                 8   \n",
       "1          7                 7         38              2                 2   \n",
       "2         57                50        320             28                 9   \n",
       "3         58                49        319             27                18   \n",
       "4         39                36        228             16                13   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                0                4       4.239130  \n",
       "1                1                2       4.571429  \n",
       "2                0                4       4.614035  \n",
       "3                0                7       4.517241  \n",
       "4                0                4       4.871795  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n",
      "Processing test...\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "punctuation = ['.', '..', '...', ',', ':', ';', '-', '*', '\"', '!', '?']\n",
    "def clean_text(x):\n",
    "    x.lower()\n",
    "    for p in punctuation:\n",
    "        x.replace(p, '')\n",
    "    return x\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text'].apply(lambda x: clean_text(x))\n",
    "test_df['text_cleaned'] = test_df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "def extract_features(df):\n",
    "    df['len'] = df['text'].apply(lambda x: len(x))\n",
    "    df['n_words'] = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "    df['n_.'] = df['text'].str.count('\\.')\n",
    "    df['n_...'] = df['text'].str.count('\\...')\n",
    "    df['n_,'] = df['text'].str.count('\\,')\n",
    "    df['n_:'] = df['text'].str.count('\\:')\n",
    "    df['n_;'] = df['text'].str.count('\\;')\n",
    "    df['n_-'] = df['text'].str.count('\\-')\n",
    "    df['n_?'] = df['text'].str.count('\\?')\n",
    "    df['n_!'] = df['text'].str.count('\\!')\n",
    "    df['n_\\''] = df['text'].str.count('\\'')\n",
    "    df['n_\"'] = df['text'].str.count('\\\"')\n",
    "\n",
    "    # 문장 첫단어 개수\n",
    "    df['n_The '] = df['text'].str.count('The ')\n",
    "    df['n_I '] = df['text'].str.count('I ')\n",
    "    df['n_It '] = df['text'].str.count('It ')\n",
    "    df['n_He '] = df['text'].str.count('He ')\n",
    "    df['n_Me '] = df['text'].str.count('Me ')\n",
    "    df['n_She '] = df['text'].str.count('She ')\n",
    "    df['n_We '] = df['text'].str.count('We ')\n",
    "    df['n_They '] = df['text'].str.count('They ')\n",
    "    df['n_You '] = df['text'].str.count('You ')\n",
    "    df['n_the'] = df['text_cleaned'].str.count('the ')\n",
    "    df['n_ a '] = df['text_cleaned'].str.count(' a ')\n",
    "    df['n_appear'] = df['text_cleaned'].str.count('appear')\n",
    "    df['n_little'] = df['text_cleaned'].str.count('little')\n",
    "    df['n_was '] = df['text_cleaned'].str.count('was ')\n",
    "    df['n_one '] = df['text_cleaned'].str.count('one ')\n",
    "    df['n_two '] = df['text_cleaned'].str.count('two ')\n",
    "    df['n_three '] = df['text_cleaned'].str.count('three ')\n",
    "    df['n_ten '] = df['text_cleaned'].str.count('ten ')\n",
    "    df['n_is '] = df['text_cleaned'].str.count('is ')\n",
    "    df['n_are '] = df['text_cleaned'].str.count('are ')\n",
    "    df['n_ed'] = df['text_cleaned'].str.count('ed ')\n",
    "    df['n_however'] = df['text_cleaned'].str.count('however')\n",
    "    df['n_ to '] = df['text_cleaned'].str.count(' to ')\n",
    "    df['n_into'] = df['text_cleaned'].str.count('into')\n",
    "    df['n_about '] = df['text_cleaned'].str.count('about ')\n",
    "    df['n_th'] = df['text_cleaned'].str.count('th')\n",
    "    df['n_er'] = df['text_cleaned'].str.count('er')\n",
    "    df['n_ex'] = df['text_cleaned'].str.count('ex')\n",
    "    df['n_an '] = df['text_cleaned'].str.count('an ')\n",
    "    df['n_ground'] = df['text_cleaned'].str.count('ground')\n",
    "    df['n_any'] = df['text_cleaned'].str.count('any')\n",
    "    df['n_silence'] = df['text_cleaned'].str.count('silence')\n",
    "    df['n_wall'] = df['text_cleaned'].str.count('wall')\n",
    "\n",
    "    df.drop(['text_cleaned'], axis=1, inplace=True)\n",
    "\n",
    "print('Processing train...')\n",
    "extract_features(train_df)\n",
    "print('Processing test...')\n",
    "extract_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos_tag와 ne_chunk를 이용한 tokenization. 자세한 내용은 https://statkclee.github.io/nlp2/nlp-ner-python.html 에 가면 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\yoons\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yoons\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\yoons\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\yoons\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 34) (19617, 34)\n",
      "(54879, 6) (19617, 6)\n",
      "(54879, 34) (19617, 34)\n",
      "(54879, 6) (19617, 6)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('punkt') #tokenization model\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tag_text(s):\n",
    "    sents = nltk.sent_tokenize(s)#문장 단위 토큰화\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        words = nltk.word_tokenize(sent)#단어 단위 토큰화\n",
    "        tag_res = [a[1] for a in nltk.pos_tag(words)]\n",
    "        res.append(' '.join(tag_res))\n",
    "    return '. '.join(res)\n",
    "\n",
    "def ne_text(s):\n",
    "    sents = nltk.sent_tokenize(s)\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tag_res = nltk.pos_tag(words)\n",
    "        ne_tree = nltk.ne_chunk(tag_res)\n",
    "        list_res = nltk.tree2conlltags(ne_tree) #Return a list of 3-tuples containing (word, tag, IOB-tag). Convert a tree to the CoNLL IOB tag format.\n",
    "        ne_res = [a[2] for a in list_res]\n",
    "        res.append(' '.join(ne_res))\n",
    "    return '. '.join(res)\n",
    "\n",
    "train_df['tag_txt'] = train_df[\"text\"].apply(pos_tag_text)\n",
    "train_df['ne_txt'] = train_df[\"text\"].apply(ne_text)\n",
    "test_df['tag_txt'] = test_df[\"text\"].apply(pos_tag_text)\n",
    "test_df['ne_txt'] = test_df[\"text\"].apply(ne_text)\n",
    "\n",
    "c_vec3 = CountVectorizer(lowercase=False)\n",
    "c_vec3.fit(train_df['tag_txt'].values.tolist())\n",
    "train_cvec3 = c_vec3.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_cvec3 = c_vec3.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_cvec3.shape,test_cvec3.shape)\n",
    "\n",
    "c_vec4 = CountVectorizer(lowercase=False)\n",
    "c_vec4.fit(train_df['ne_txt'].values.tolist())\n",
    "train_cvec4 = c_vec4.transform(train_df['ne_txt'].values.tolist()).toarray()\n",
    "test_cvec4 = c_vec4.transform(test_df['ne_txt'].values.tolist()).toarray()\n",
    "print(train_cvec4.shape,test_cvec4.shape)\n",
    "\n",
    "tf_vec5 = TfidfVectorizer(lowercase=False)\n",
    "tf_vec5.fit(train_df['tag_txt'].values.tolist())\n",
    "train_tf5 = tf_vec5.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_tf5 = tf_vec5.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_tf5.shape,test_tf5.shape)\n",
    "\n",
    "tf_vec6 = TfidfVectorizer(lowercase=False)\n",
    "tf_vec6.fit(train_df['ne_txt'].values.tolist())\n",
    "train_tf6 = tf_vec6.transform(train_df['ne_txt'].values.tolist()).toarray()\n",
    "test_tf6 = tf_vec6.transform(test_df['ne_txt'].values.tolist()).toarray()\n",
    "print(train_tf6.shape,test_tf6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS : Part of Speech (품사) 낱말을 문법적인 기능이나 형태, 뜻에 따라 구분**\n",
    "\n",
    "\n",
    "* NNP: 단수 고유명사\n",
    "\n",
    "* VB: 동사\n",
    "\n",
    "* VBP: 동사 현재형\n",
    "\n",
    "* TO: to 전치사\n",
    "\n",
    "* NN: 명사(단수형 혹은 집합형)\n",
    "\n",
    "* DT: 관형사\n",
    "\n",
    "**개체명 인식(Named Entity Recognition)**  \n",
    "nltk ne_chunk는 개체명을 태깅하기 위해서 앞서 품사 태깅(pos_tag)이 수행되어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB을 이용한 feature 생성\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2137725) (19617, 2137725)\n",
      "(54879, 30) (19617, 30)\n",
      "(54879, 2485843) (19617, 2485843)\n",
      "(54879, 30) (19617, 30)\n",
      "(54879, 2137725) (19617, 2137725)\n",
      "(54879, 2485843) (19617, 2485843)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\yoons\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass alpha=0.025 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "train_Y = train_df['author']\n",
    "train_id = train_df['index'].values\n",
    "test_id = test_df['index'].values\n",
    "\n",
    "# tfidf와 svd 합\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "#ngram_range : 단어의 묶음 갯수 범위(여기서는 1~3)\n",
    "#max_df : 문서의 80% 이상 나타나는 단어 무시\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist()) #세개로 나누는 이유?\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1 특이값 구하기\n",
    "n_comp = 30 #desired dimensionality\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack') #arpack or randomnized solver 중 택1\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "# tfidf char\n",
    "tfidf_vec2 = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True) #Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "full_tfidf2 = tfidf_vec2.fit_transform(train_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec2.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "# svd2\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "\n",
    "# cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n",
    "c_vec.fit(train_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "# cnt char\n",
    "c_vec2 = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "c_vec2.fit(train_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec2.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "\n",
    "def gen_nb_feats(rnd=1): #split한 데이터 4개 단어, 문자열 tfid, countvec 4개에 관한 각각의 probability feature 저장\n",
    "    help_tfidf_train,help_tfidf_test = np.zeros((54879,5)),np.zeros((19617,5))\n",
    "    help_tfidf_train2,help_tfidf_test2 = np.zeros((54879,5)),np.zeros((19617,5))\n",
    "    help_cnt1_train,help_cnt1_test = np.zeros((54879,5)),np.zeros((19617,5))\n",
    "    help_cnt2_train,help_cnt2_test = np.zeros((54879,5)),np.zeros((19617,5))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in skf.split(train_tfidf,train_Y):\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = MultinomialNB(alpha=0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "        help_tfidf_train[test_index] = tmp_train_feat\n",
    "        help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "        help_tfidf_train2[test_index] = tmp_train_feat\n",
    "        help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec to nb\n",
    "        X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "        help_cnt1_train[test_index] = tmp_train_feat\n",
    "        help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec2 to nb \n",
    "        X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "        help_cnt2_train[test_index] = tmp_train_feat\n",
    "        help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    help_train_feat = np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train])\n",
    "    help_test_feat = np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test])\n",
    "\n",
    "    return help_train_feat,help_test_feat\n",
    "    \n",
    "help_train_feat,help_test_feat = gen_nb_feats(1)\n",
    "help_train_feat2,help_test_feat2 = gen_nb_feats(2)\n",
    "help_train_feat3,help_test_feat3 = gen_nb_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.77899127e-004, 3.96783464e-004, 2.47808992e-004, ...,\n",
       "        2.75590524e-165, 1.00000000e+000, 1.11635484e-193],\n",
       "       [2.86399325e-001, 2.72703287e-001, 2.21921410e-002, ...,\n",
       "        5.67000591e-041, 4.82436037e-011, 1.68177235e-029],\n",
       "       [1.10424591e-002, 9.84788634e-001, 9.65152662e-004, ...,\n",
       "        0.00000000e+000, 1.21645782e-279, 0.00000000e+000],\n",
       "       ...,\n",
       "       [2.54342981e-001, 4.31336000e-001, 7.98645952e-002, ...,\n",
       "        5.77920372e-070, 7.98348157e-079, 2.98668386e-069],\n",
       "       [1.57985002e-001, 2.87089382e-002, 1.31857247e-001, ...,\n",
       "        2.94437644e-042, 1.00000000e+000, 6.07069844e-044],\n",
       "       [6.68730690e-001, 2.01489255e-001, 3.01592026e-002, ...,\n",
       "        1.40936741e-028, 2.65719052e-004, 2.86506563e-042]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help_train_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, GRU, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN을 이용한 feature 생성\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_feats(rnd=1):\n",
    "    train_pred, test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 150\n",
    "    NUM_CLASSES = 5\n",
    "    MODEL_P = 'nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS) #30000개의 단어 토큰화\n",
    "    tokenizer.fit_on_texts(tmp_X) #Updates internal vocabulary based on a list of texts.\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X) #Transforms each text in texts to a sequence of integers.\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN) #시퀀스 패딩으로 길이 맞추는 과정 MAX_LEN(150)으로 설정\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in skf.split(train_tfidf,tmp_Y):\n",
    "        model = Sequential() #keras sequential() 모델\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))#첫번째 인자 = 단어 집합의 크기. 즉, 총 단어의 개수 두번째 인자 = 임베딩 벡터의 출력 차원. 결과로서 나오는 임베딩 벡터의 크기input_length = 입력 시퀀스의 길이\n",
    "        model.add(Conv1D(16,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=64, epochs=15, \n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_cnn_feats 함수에서 인자는 단순히 seed값 변경의 의미만을 가지기 때문에, 굳이 3번이나 반복해야하나 싶어 feature 생성을 한번만 하였더니 이 또한 정확도 하락에 기여하였습니다. 이해할순 없지만 이러한 앙상블 역시 정확도에 기여하는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "618/618 [==============================] - 7s 10ms/step - loss: 1.5611 - accuracy: 0.2728 - val_loss: 1.4201 - val_accuracy: 0.3594\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.42006, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.3860 - accuracy: 0.3901 - val_loss: 1.2571 - val_accuracy: 0.4509\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.42006 to 1.25710, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.2123 - accuracy: 0.4679 - val_loss: 1.1523 - val_accuracy: 0.5129\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.25710 to 1.15234, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.0693 - accuracy: 0.5433 - val_loss: 1.0786 - val_accuracy: 0.5862\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.15234 to 1.07863, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.9635 - accuracy: 0.6017 - val_loss: 1.0345 - val_accuracy: 0.6085\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.07863 to 1.03446, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.8718 - accuracy: 0.6513 - val_loss: 1.0054 - val_accuracy: 0.6229\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.03446 to 1.00536, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.8051 - accuracy: 0.6816 - val_loss: 0.9880 - val_accuracy: 0.6352\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.00536 to 0.98797, saving model to nn_model.h5\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.7426 - accuracy: 0.7125 - val_loss: 0.9777 - val_accuracy: 0.6495\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.98797 to 0.97773, saving model to nn_model.h5\n",
      "Epoch 9/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6849 - accuracy: 0.7396 - val_loss: 0.9740 - val_accuracy: 0.6575\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.97773 to 0.97396, saving model to nn_model.h5\n",
      "Epoch 10/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.6430 - accuracy: 0.7558 - val_loss: 0.9831 - val_accuracy: 0.6652\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.97396\n",
      "Epoch 11/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.5938 - accuracy: 0.7780 - val_loss: 0.9805 - val_accuracy: 0.6741\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.97396\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.5471 - accuracy: 0.2894 - val_loss: 1.2395 - val_accuracy: 0.4826\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.23954, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 1.1756 - accuracy: 0.4985 - val_loss: 1.0753 - val_accuracy: 0.5529\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.23954 to 1.07525, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 1.0128 - accuracy: 0.5679 - val_loss: 1.0238 - val_accuracy: 0.5712\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.07525 to 1.02375, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.9145 - accuracy: 0.6163 - val_loss: 0.9901 - val_accuracy: 0.6074\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.02375 to 0.99011, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.8374 - accuracy: 0.6592 - val_loss: 0.9753 - val_accuracy: 0.6258\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.99011 to 0.97533, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.7626 - accuracy: 0.6955 - val_loss: 0.9675 - val_accuracy: 0.6331\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.97533 to 0.96750, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.6999 - accuracy: 0.7298 - val_loss: 0.9629 - val_accuracy: 0.6388\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.96750 to 0.96289, saving model to nn_model.h5\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6412 - accuracy: 0.7595 - val_loss: 0.9715 - val_accuracy: 0.6468\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.96289\n",
      "Epoch 9/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5908 - accuracy: 0.7825 - val_loss: 0.9769 - val_accuracy: 0.6554\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.96289\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.5502 - accuracy: 0.2853 - val_loss: 1.2716 - val_accuracy: 0.4739\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.27159, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 1.2011 - accuracy: 0.5048 - val_loss: 1.0440 - val_accuracy: 0.5826\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.27159 to 1.04405, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.9754 - accuracy: 0.6110 - val_loss: 0.9553 - val_accuracy: 0.6256\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.04405 to 0.95530, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.8464 - accuracy: 0.6640 - val_loss: 0.9179 - val_accuracy: 0.6436\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.95530 to 0.91792, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7568 - accuracy: 0.7002 - val_loss: 0.8979 - val_accuracy: 0.6616\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.91792 to 0.89787, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6840 - accuracy: 0.7431 - val_loss: 0.8841 - val_accuracy: 0.6737\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.89787 to 0.88412, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6182 - accuracy: 0.7746 - val_loss: 0.8855 - val_accuracy: 0.6809\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.88412\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5603 - accuracy: 0.7985 - val_loss: 0.8969 - val_accuracy: 0.6887\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.88412\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.5527 - accuracy: 0.2884 - val_loss: 1.2944 - val_accuracy: 0.4682\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.29445, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.1910 - accuracy: 0.5064 - val_loss: 1.0599 - val_accuracy: 0.5705\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.29445 to 1.05993, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.9748 - accuracy: 0.6073 - val_loss: 0.9791 - val_accuracy: 0.6037\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.05993 to 0.97906, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.8547 - accuracy: 0.6598 - val_loss: 0.9519 - val_accuracy: 0.6172\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.97906 to 0.95189, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7730 - accuracy: 0.6913 - val_loss: 0.9465 - val_accuracy: 0.6235\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.95189 to 0.94646, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7100 - accuracy: 0.7113 - val_loss: 0.9579 - val_accuracy: 0.6249\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.94646\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.6573 - accuracy: 0.7310 - val_loss: 0.9719 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.94646\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.5664 - accuracy: 0.2818 - val_loss: 1.3487 - val_accuracy: 0.4359\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.34867, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 1.2294 - accuracy: 0.5060 - val_loss: 1.0294 - val_accuracy: 0.5951\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.34867 to 1.02941, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.9454 - accuracy: 0.6407 - val_loss: 0.8803 - val_accuracy: 0.6696\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02941 to 0.88026, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.7651 - accuracy: 0.7216 - val_loss: 0.7939 - val_accuracy: 0.7067\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.88026 to 0.79385, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6561 - accuracy: 0.7659 - val_loss: 0.7535 - val_accuracy: 0.7203\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.79385 to 0.75352, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.5669 - accuracy: 0.8006 - val_loss: 0.7500 - val_accuracy: 0.7304\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.75352 to 0.75002, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.5054 - accuracy: 0.8256 - val_loss: 0.7514 - val_accuracy: 0.7333\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.75002\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.4575 - accuracy: 0.8429 - val_loss: 0.7692 - val_accuracy: 0.7301\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.75002\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.5526 - accuracy: 0.2967 - val_loss: 1.2429 - val_accuracy: 0.4808\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.24291, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.1689 - accuracy: 0.5174 - val_loss: 1.0284 - val_accuracy: 0.5700\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.24291 to 1.02836, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.9747 - accuracy: 0.6016 - val_loss: 0.9529 - val_accuracy: 0.6119\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02836 to 0.95288, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.8673 - accuracy: 0.6540 - val_loss: 0.9176 - val_accuracy: 0.6361\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.95288 to 0.91762, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7847 - accuracy: 0.6938 - val_loss: 0.8993 - val_accuracy: 0.6493\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.91762 to 0.89927, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7185 - accuracy: 0.7265 - val_loss: 0.8950 - val_accuracy: 0.6557\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.89927 to 0.89496, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6593 - accuracy: 0.7549 - val_loss: 0.8977 - val_accuracy: 0.6591\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.89496\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6112 - accuracy: 0.7780 - val_loss: 0.9080 - val_accuracy: 0.6661\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.89496\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.5669 - accuracy: 0.2783 - val_loss: 1.3806 - val_accuracy: 0.3901\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.38063, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 1.2941 - accuracy: 0.4594 - val_loss: 1.1197 - val_accuracy: 0.5470\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.38063 to 1.11970, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 1.0463 - accuracy: 0.5743 - val_loss: 1.0014 - val_accuracy: 0.5889\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.11970 to 1.00137, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.9017 - accuracy: 0.6315 - val_loss: 0.9686 - val_accuracy: 0.6005\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00137 to 0.96856, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.8116 - accuracy: 0.6768 - val_loss: 0.9617 - val_accuracy: 0.6174\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.96856 to 0.96168, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7298 - accuracy: 0.7163 - val_loss: 0.9686 - val_accuracy: 0.6320\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.96168\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6700 - accuracy: 0.7449 - val_loss: 0.9849 - val_accuracy: 0.6352\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.96168\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.5528 - accuracy: 0.2794 - val_loss: 1.3240 - val_accuracy: 0.4398\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.32401, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.2552 - accuracy: 0.4636 - val_loss: 1.0845 - val_accuracy: 0.5593\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.32401 to 1.08449, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.0084 - accuracy: 0.5940 - val_loss: 0.9665 - val_accuracy: 0.6215\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.08449 to 0.96652, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.8580 - accuracy: 0.6619 - val_loss: 0.9183 - val_accuracy: 0.6436\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.96652 to 0.91831, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7613 - accuracy: 0.7078 - val_loss: 0.9048 - val_accuracy: 0.6573\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.91831 to 0.90483, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6927 - accuracy: 0.7415 - val_loss: 0.9123 - val_accuracy: 0.6607\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.90483\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6256 - accuracy: 0.7694 - val_loss: 0.9353 - val_accuracy: 0.6614\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.90483\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.5638 - accuracy: 0.2732 - val_loss: 1.4292 - val_accuracy: 0.3735\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.42920, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.3660 - accuracy: 0.4134 - val_loss: 1.1491 - val_accuracy: 0.5605\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.42920 to 1.14907, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.0719 - accuracy: 0.5872 - val_loss: 0.9809 - val_accuracy: 0.6256\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.14907 to 0.98093, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.8771 - accuracy: 0.6735 - val_loss: 0.9309 - val_accuracy: 0.6429\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.98093 to 0.93088, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7636 - accuracy: 0.7201 - val_loss: 0.9120 - val_accuracy: 0.6545\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.93088 to 0.91202, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.6814 - accuracy: 0.7548 - val_loss: 0.9105 - val_accuracy: 0.6618\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.91202 to 0.91054, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.6190 - accuracy: 0.7815 - val_loss: 0.9157 - val_accuracy: 0.6696\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.91054\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5701 - accuracy: 0.8009 - val_loss: 0.9390 - val_accuracy: 0.6682\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.91054\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.5547 - accuracy: 0.2913 - val_loss: 1.3000 - val_accuracy: 0.4377\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.29998, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.2415 - accuracy: 0.4718 - val_loss: 1.1367 - val_accuracy: 0.5231\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.29998 to 1.13672, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 6s 9ms/step - loss: 1.0685 - accuracy: 0.5535 - val_loss: 1.0576 - val_accuracy: 0.5625\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.13672 to 1.05763, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 6s 10ms/step - loss: 0.9600 - accuracy: 0.6094 - val_loss: 1.0166 - val_accuracy: 0.5878\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.05763 to 1.01660, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 6s 10ms/step - loss: 0.8724 - accuracy: 0.6463 - val_loss: 0.9875 - val_accuracy: 0.6024\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.01660 to 0.98750, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 6s 9ms/step - loss: 0.7998 - accuracy: 0.6749 - val_loss: 0.9720 - val_accuracy: 0.6201\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.98750 to 0.97199, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7393 - accuracy: 0.7006 - val_loss: 0.9664 - val_accuracy: 0.6331\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.97199 to 0.96639, saving model to nn_model.h5\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6826 - accuracy: 0.7281 - val_loss: 0.9631 - val_accuracy: 0.6415\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.96639 to 0.96306, saving model to nn_model.h5\n",
      "Epoch 9/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6313 - accuracy: 0.7547 - val_loss: 0.9749 - val_accuracy: 0.6527\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.96306\n",
      "Epoch 10/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5827 - accuracy: 0.7782 - val_loss: 0.9816 - val_accuracy: 0.6655\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.96306\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.5684 - accuracy: 0.2793 - val_loss: 1.3214 - val_accuracy: 0.4532\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.32139, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.2471 - accuracy: 0.4884 - val_loss: 1.0576 - val_accuracy: 0.5896\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.32139 to 1.05764, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.9890 - accuracy: 0.6127 - val_loss: 0.9431 - val_accuracy: 0.6297\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.05764 to 0.94307, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.8450 - accuracy: 0.6783 - val_loss: 0.8912 - val_accuracy: 0.6559\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.94307 to 0.89123, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7491 - accuracy: 0.7166 - val_loss: 0.8661 - val_accuracy: 0.6684\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.89123 to 0.86611, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6709 - accuracy: 0.7512 - val_loss: 0.8558 - val_accuracy: 0.6821\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.86611 to 0.85580, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5975 - accuracy: 0.7832 - val_loss: 0.8440 - val_accuracy: 0.6944\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.85580 to 0.84395, saving model to nn_model.h5\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5466 - accuracy: 0.8073 - val_loss: 0.8687 - val_accuracy: 0.6989\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.84395\n",
      "Epoch 9/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5048 - accuracy: 0.8222 - val_loss: 0.8570 - val_accuracy: 0.7087\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.84395\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 6s 8ms/step - loss: 1.5505 - accuracy: 0.3014 - val_loss: 1.2407 - val_accuracy: 0.4735\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.24074, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.1994 - accuracy: 0.5005 - val_loss: 1.0859 - val_accuracy: 0.5409\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.24074 to 1.08593, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.0280 - accuracy: 0.5728 - val_loss: 1.0067 - val_accuracy: 0.5801\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.08593 to 1.00666, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.9142 - accuracy: 0.6297 - val_loss: 0.9724 - val_accuracy: 0.6083\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.00666 to 0.97245, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.8371 - accuracy: 0.6639 - val_loss: 0.9599 - val_accuracy: 0.6194\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.97245 to 0.95995, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7691 - accuracy: 0.6912 - val_loss: 0.9411 - val_accuracy: 0.6434\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.95995 to 0.94109, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7021 - accuracy: 0.7276 - val_loss: 0.9251 - val_accuracy: 0.6609\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.94109 to 0.92513, saving model to nn_model.h5\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6393 - accuracy: 0.7588 - val_loss: 0.9180 - val_accuracy: 0.6777\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.92513 to 0.91798, saving model to nn_model.h5\n",
      "Epoch 9/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5829 - accuracy: 0.7856 - val_loss: 0.9032 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.91798 to 0.90316, saving model to nn_model.h5\n",
      "Epoch 10/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.5262 - accuracy: 0.8087 - val_loss: 0.9091 - val_accuracy: 0.6969\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.90316\n",
      "Epoch 11/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.4839 - accuracy: 0.8296 - val_loss: 0.9297 - val_accuracy: 0.7049\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.90316\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.5620 - accuracy: 0.2771 - val_loss: 1.2909 - val_accuracy: 0.4990\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.29088, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.1928 - accuracy: 0.5454 - val_loss: 0.9592 - val_accuracy: 0.6495\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.29088 to 0.95917, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.9281 - accuracy: 0.6524 - val_loss: 0.8597 - val_accuracy: 0.6809\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.95917 to 0.85965, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7943 - accuracy: 0.7071 - val_loss: 0.8116 - val_accuracy: 0.6951\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.85965 to 0.81160, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6978 - accuracy: 0.7419 - val_loss: 0.7857 - val_accuracy: 0.7046\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.81160 to 0.78574, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6239 - accuracy: 0.7718 - val_loss: 0.7736 - val_accuracy: 0.7110\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.78574 to 0.77356, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5631 - accuracy: 0.7960 - val_loss: 0.7798 - val_accuracy: 0.7133\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.77356\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 0.5134 - accuracy: 0.8159 - val_loss: 0.7788 - val_accuracy: 0.7219\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.77356\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 6s 8ms/step - loss: 1.5555 - accuracy: 0.2804 - val_loss: 1.3752 - val_accuracy: 0.3698\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.37517, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.3301 - accuracy: 0.4044 - val_loss: 1.2069 - val_accuracy: 0.4443\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.37517 to 1.20691, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.1314 - accuracy: 0.5129 - val_loss: 1.0716 - val_accuracy: 0.5550\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.20691 to 1.07163, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.9752 - accuracy: 0.5970 - val_loss: 0.9775 - val_accuracy: 0.6147\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.07163 to 0.97753, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.8511 - accuracy: 0.6602 - val_loss: 0.9148 - val_accuracy: 0.6545\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.97753 to 0.91478, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7486 - accuracy: 0.7123 - val_loss: 0.8713 - val_accuracy: 0.6702\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.91478 to 0.87135, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6640 - accuracy: 0.7511 - val_loss: 0.8508 - val_accuracy: 0.6885\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.87135 to 0.85080, saving model to nn_model.h5\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5929 - accuracy: 0.7864 - val_loss: 0.8327 - val_accuracy: 0.7028\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.85080 to 0.83270, saving model to nn_model.h5\n",
      "Epoch 9/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5399 - accuracy: 0.8080 - val_loss: 0.8291 - val_accuracy: 0.7115\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.83270 to 0.82906, saving model to nn_model.h5\n",
      "Epoch 10/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.4981 - accuracy: 0.8230 - val_loss: 0.8396 - val_accuracy: 0.7126\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.82906\n",
      "Epoch 11/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.4613 - accuracy: 0.8397 - val_loss: 0.8532 - val_accuracy: 0.7174\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.82906\n",
      "------------------\n",
      "Epoch 1/15\n",
      "618/618 [==============================] - 6s 9ms/step - loss: 1.5597 - accuracy: 0.2794 - val_loss: 1.3940 - val_accuracy: 0.3992\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.39395, saving model to nn_model.h5\n",
      "Epoch 2/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.3337 - accuracy: 0.4232 - val_loss: 1.2219 - val_accuracy: 0.4826\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.39395 to 1.22187, saving model to nn_model.h5\n",
      "Epoch 3/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 1.1488 - accuracy: 0.5157 - val_loss: 1.1151 - val_accuracy: 0.5500\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.22187 to 1.11507, saving model to nn_model.h5\n",
      "Epoch 4/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.9943 - accuracy: 0.5961 - val_loss: 1.0365 - val_accuracy: 0.5926\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.11507 to 1.03648, saving model to nn_model.h5\n",
      "Epoch 5/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.8712 - accuracy: 0.6480 - val_loss: 0.9949 - val_accuracy: 0.6142\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.03648 to 0.99493, saving model to nn_model.h5\n",
      "Epoch 6/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7819 - accuracy: 0.6885 - val_loss: 0.9669 - val_accuracy: 0.6384\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.99493 to 0.96694, saving model to nn_model.h5\n",
      "Epoch 7/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.7037 - accuracy: 0.7287 - val_loss: 0.9484 - val_accuracy: 0.6579\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.96694 to 0.94837, saving model to nn_model.h5\n",
      "Epoch 8/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.6332 - accuracy: 0.7667 - val_loss: 0.9435 - val_accuracy: 0.6777\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.94837 to 0.94350, saving model to nn_model.h5\n",
      "Epoch 9/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5661 - accuracy: 0.7946 - val_loss: 0.9374 - val_accuracy: 0.6923\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.94350 to 0.93743, saving model to nn_model.h5\n",
      "Epoch 10/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.5169 - accuracy: 0.8177 - val_loss: 0.9539 - val_accuracy: 0.6957\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.93743\n",
      "Epoch 11/15\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 0.4685 - accuracy: 0.8370 - val_loss: 0.9785 - val_accuracy: 0.6978\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.93743\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "cnn_train1,cnn_test1,cnn_train2,cnn_test2 = get_cnn_feats(1)\n",
    "cnn_train3,cnn_test3,cnn_train4,cnn_test4 = get_cnn_feats(2)\n",
    "cnn_train5,cnn_test5,cnn_train6,cnn_test6 = get_cnn_feats(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU를 이용한 feature 생성\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gru_feats(rnd=1):\n",
    "    train_pred, test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 16000\n",
    "    N = 12\n",
    "    MAX_LEN = 300\n",
    "    NUM_CLASSES = 5\n",
    "    MODEL_P = 'nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    for train_index, test_index in skf.split(ttrain_x,tmp_Y):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(GRU(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es=EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=256, epochs=10, \n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뒤늦게 안 사실인데 해당 데이터에서는 GRU보다 LSTM이 마지막 앙상블에서 더 성능이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "155/155 [==============================] - 33s 197ms/step - loss: 1.5058 - accuracy: 0.3133 - val_loss: 1.0251 - val_accuracy: 0.5880\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02513, saving model to nn_model.h5\n",
      "Epoch 2/10\n",
      "155/155 [==============================] - 29s 188ms/step - loss: 0.9770 - accuracy: 0.6050 - val_loss: 0.8404 - val_accuracy: 0.6750\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02513 to 0.84039, saving model to nn_model.h5\n",
      "Epoch 3/10\n",
      "155/155 [==============================] - 30s 195ms/step - loss: 0.7602 - accuracy: 0.7056 - val_loss: 0.7804 - val_accuracy: 0.6962\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.84039 to 0.78040, saving model to nn_model.h5\n",
      "Epoch 4/10\n",
      "155/155 [==============================] - 32s 209ms/step - loss: 0.6258 - accuracy: 0.7663 - val_loss: 0.7551 - val_accuracy: 0.7176\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.78040 to 0.75506, saving model to nn_model.h5\n",
      "Epoch 5/10\n",
      "155/155 [==============================] - 33s 210ms/step - loss: 0.5172 - accuracy: 0.8123 - val_loss: 0.7227 - val_accuracy: 0.7285\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.75506 to 0.72268, saving model to nn_model.h5\n",
      "Epoch 6/10\n",
      "155/155 [==============================] - 33s 212ms/step - loss: 0.4440 - accuracy: 0.8363 - val_loss: 0.6977 - val_accuracy: 0.7490\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.72268 to 0.69765, saving model to nn_model.h5\n",
      "Epoch 7/10\n",
      "155/155 [==============================] - 33s 213ms/step - loss: 0.3883 - accuracy: 0.8593 - val_loss: 0.7044 - val_accuracy: 0.7513\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69765\n",
      "Epoch 8/10\n",
      "155/155 [==============================] - 32s 209ms/step - loss: 0.3479 - accuracy: 0.8744 - val_loss: 0.7333 - val_accuracy: 0.7529\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69765\n",
      "------------------\n",
      "Epoch 1/10\n",
      "155/155 [==============================] - 36s 217ms/step - loss: 1.5040 - accuracy: 0.3219 - val_loss: 1.0259 - val_accuracy: 0.5942\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02588, saving model to nn_model.h5\n",
      "Epoch 2/10\n",
      "155/155 [==============================] - 33s 210ms/step - loss: 0.9284 - accuracy: 0.6412 - val_loss: 0.7550 - val_accuracy: 0.7074\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.02588 to 0.75500, saving model to nn_model.h5\n",
      "Epoch 3/10\n",
      "155/155 [==============================] - 33s 210ms/step - loss: 0.6387 - accuracy: 0.7625 - val_loss: 0.6546 - val_accuracy: 0.7524\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.75500 to 0.65455, saving model to nn_model.h5\n",
      "Epoch 4/10\n",
      "155/155 [==============================] - 33s 211ms/step - loss: 0.4980 - accuracy: 0.8205 - val_loss: 0.6568 - val_accuracy: 0.7606\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.65455\n",
      "Epoch 5/10\n",
      "155/155 [==============================] - 32s 209ms/step - loss: 0.4201 - accuracy: 0.8466 - val_loss: 0.6877 - val_accuracy: 0.7588\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.65455\n",
      "------------------\n",
      "Epoch 1/10\n",
      "155/155 [==============================] - 35s 211ms/step - loss: 1.4922 - accuracy: 0.3370 - val_loss: 1.0016 - val_accuracy: 0.6081\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00162, saving model to nn_model.h5\n",
      "Epoch 2/10\n",
      "155/155 [==============================] - 34s 218ms/step - loss: 0.9168 - accuracy: 0.6433 - val_loss: 0.7786 - val_accuracy: 0.7035\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00162 to 0.77855, saving model to nn_model.h5\n",
      "Epoch 3/10\n",
      "155/155 [==============================] - 34s 216ms/step - loss: 0.6822 - accuracy: 0.7422 - val_loss: 0.7316 - val_accuracy: 0.7206\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.77855 to 0.73163, saving model to nn_model.h5\n",
      "Epoch 4/10\n",
      "155/155 [==============================] - 34s 217ms/step - loss: 0.5599 - accuracy: 0.7931 - val_loss: 0.7192 - val_accuracy: 0.7290\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.73163 to 0.71923, saving model to nn_model.h5\n",
      "Epoch 5/10\n",
      "155/155 [==============================] - 34s 217ms/step - loss: 0.4795 - accuracy: 0.8248 - val_loss: 0.7117 - val_accuracy: 0.7390\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.71923 to 0.71168, saving model to nn_model.h5\n",
      "Epoch 6/10\n",
      "155/155 [==============================] - 34s 217ms/step - loss: 0.4296 - accuracy: 0.8434 - val_loss: 0.7214 - val_accuracy: 0.7461\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.71168\n",
      "Epoch 7/10\n",
      "155/155 [==============================] - 34s 218ms/step - loss: 0.3899 - accuracy: 0.8609 - val_loss: 0.7377 - val_accuracy: 0.7468\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.71168\n",
      "------------------\n",
      "Epoch 1/10\n",
      "155/155 [==============================] - 37s 219ms/step - loss: 1.5103 - accuracy: 0.3080 - val_loss: 1.1428 - val_accuracy: 0.5336\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.14277, saving model to nn_model.h5\n",
      "Epoch 2/10\n",
      "155/155 [==============================] - 34s 217ms/step - loss: 1.0278 - accuracy: 0.5898 - val_loss: 0.8315 - val_accuracy: 0.6743\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.14277 to 0.83146, saving model to nn_model.h5\n",
      "Epoch 3/10\n",
      "155/155 [==============================] - 34s 218ms/step - loss: 0.7353 - accuracy: 0.7178 - val_loss: 0.7826 - val_accuracy: 0.6980\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.83146 to 0.78257, saving model to nn_model.h5\n",
      "Epoch 4/10\n",
      "155/155 [==============================] - 34s 217ms/step - loss: 0.5972 - accuracy: 0.7745 - val_loss: 0.7250 - val_accuracy: 0.7263\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.78257 to 0.72501, saving model to nn_model.h5\n",
      "Epoch 5/10\n",
      "155/155 [==============================] - 34s 217ms/step - loss: 0.5041 - accuracy: 0.8136 - val_loss: 0.7301 - val_accuracy: 0.7338\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.72501\n",
      "Epoch 6/10\n",
      "155/155 [==============================] - 34s 216ms/step - loss: 0.4400 - accuracy: 0.8404 - val_loss: 0.7501 - val_accuracy: 0.7404\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.72501\n",
      "------------------\n",
      "Epoch 1/10\n",
      "155/155 [==============================] - 36s 219ms/step - loss: 1.5095 - accuracy: 0.3141 - val_loss: 1.0195 - val_accuracy: 0.5921\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01953, saving model to nn_model.h5\n",
      "Epoch 2/10\n",
      "155/155 [==============================] - 33s 214ms/step - loss: 0.9324 - accuracy: 0.6364 - val_loss: 0.7650 - val_accuracy: 0.7060\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01953 to 0.76504, saving model to nn_model.h5\n",
      "Epoch 3/10\n",
      "155/155 [==============================] - 33s 213ms/step - loss: 0.6812 - accuracy: 0.7489 - val_loss: 0.7144 - val_accuracy: 0.7249\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.76504 to 0.71437, saving model to nn_model.h5\n",
      "Epoch 4/10\n",
      "155/155 [==============================] - 33s 213ms/step - loss: 0.5490 - accuracy: 0.7995 - val_loss: 0.6835 - val_accuracy: 0.7502\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.71437 to 0.68345, saving model to nn_model.h5\n",
      "Epoch 5/10\n",
      "155/155 [==============================] - 33s 213ms/step - loss: 0.4515 - accuracy: 0.8362 - val_loss: 0.7079 - val_accuracy: 0.7474\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.68345\n",
      "Epoch 6/10\n",
      "155/155 [==============================] - 34s 218ms/step - loss: 0.4015 - accuracy: 0.8560 - val_loss: 0.7196 - val_accuracy: 0.7520\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.68345\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "gru_train1,gru_test1,gru_train2,gru_test2 = get_gru_feats(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN을 이용한 feature 생성\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "541/541 [==============================] - 4s 7ms/step - loss: 1.5648 - accuracy: 0.2815 - val_loss: 1.3790 - val_accuracy: 0.4383\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.37900, saving model to nn_model.h5\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 1.2833 - accuracy: 0.4857 - val_loss: 1.0982 - val_accuracy: 0.5694\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.37900 to 1.09817, saving model to nn_model.h5\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 1.0418 - accuracy: 0.5866 - val_loss: 0.9616 - val_accuracy: 0.6356\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.09817 to 0.96162, saving model to nn_model.h5\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.8891 - accuracy: 0.6621 - val_loss: 0.8720 - val_accuracy: 0.6730\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.96162 to 0.87204, saving model to nn_model.h5\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.7763 - accuracy: 0.7083 - val_loss: 0.8109 - val_accuracy: 0.6976\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.87204 to 0.81090, saving model to nn_model.h5\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.6822 - accuracy: 0.7477 - val_loss: 0.7635 - val_accuracy: 0.7203\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.81090 to 0.76347, saving model to nn_model.h5\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.6023 - accuracy: 0.7852 - val_loss: 0.7289 - val_accuracy: 0.7369\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.76347 to 0.72889, saving model to nn_model.h5\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.5294 - accuracy: 0.8127 - val_loss: 0.7032 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.72889 to 0.70322, saving model to nn_model.h5\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4728 - accuracy: 0.8365 - val_loss: 0.6883 - val_accuracy: 0.7569\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.70322 to 0.68833, saving model to nn_model.h5\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.4235 - accuracy: 0.8536 - val_loss: 0.6802 - val_accuracy: 0.7625\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.68833 to 0.68015, saving model to nn_model.h5\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3849 - accuracy: 0.8681 - val_loss: 0.6779 - val_accuracy: 0.7634\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.68015 to 0.67791, saving model to nn_model.h5\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3502 - accuracy: 0.8821 - val_loss: 0.6805 - val_accuracy: 0.7664\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.67791\n",
      "Epoch 13/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3219 - accuracy: 0.8918 - val_loss: 0.6888 - val_accuracy: 0.7656\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.67791\n",
      "------------------\n",
      "Epoch 1/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.5602 - accuracy: 0.2840 - val_loss: 1.4049 - val_accuracy: 0.4447\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.40494, saving model to nn_model.h5\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 1.3216 - accuracy: 0.4855 - val_loss: 1.0900 - val_accuracy: 0.5879\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.40494 to 1.08997, saving model to nn_model.h5\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 1.0151 - accuracy: 0.6273 - val_loss: 0.9234 - val_accuracy: 0.6423\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.08997 to 0.92337, saving model to nn_model.h5\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.8389 - accuracy: 0.6858 - val_loss: 0.8364 - val_accuracy: 0.6816\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.92337 to 0.83643, saving model to nn_model.h5\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.7269 - accuracy: 0.7306 - val_loss: 0.7814 - val_accuracy: 0.7081\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.83643 to 0.78137, saving model to nn_model.h5\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.6395 - accuracy: 0.7675 - val_loss: 0.7447 - val_accuracy: 0.7252\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.78137 to 0.74470, saving model to nn_model.h5\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.5680 - accuracy: 0.7957 - val_loss: 0.7232 - val_accuracy: 0.7344\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.74470 to 0.72323, saving model to nn_model.h5\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.5035 - accuracy: 0.8233 - val_loss: 0.7094 - val_accuracy: 0.7407\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.72323 to 0.70943, saving model to nn_model.h5\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4537 - accuracy: 0.8426 - val_loss: 0.7029 - val_accuracy: 0.7467\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.70943 to 0.70286, saving model to nn_model.h5\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4094 - accuracy: 0.8573 - val_loss: 0.7025 - val_accuracy: 0.7499\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.70286 to 0.70250, saving model to nn_model.h5\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3720 - accuracy: 0.8724 - val_loss: 0.7084 - val_accuracy: 0.7511\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.70250\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3415 - accuracy: 0.8842 - val_loss: 0.7126 - val_accuracy: 0.7550\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.70250\n",
      "------------------\n",
      "Epoch 1/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.5626 - accuracy: 0.2955 - val_loss: 1.4117 - val_accuracy: 0.5030\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.41170, saving model to nn_model.h5\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 1.2982 - accuracy: 0.5291 - val_loss: 1.0280 - val_accuracy: 0.6166\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.41170 to 1.02796, saving model to nn_model.h5\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.9662 - accuracy: 0.6352 - val_loss: 0.8927 - val_accuracy: 0.6599\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02796 to 0.89275, saving model to nn_model.h5\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.8182 - accuracy: 0.6895 - val_loss: 0.8200 - val_accuracy: 0.6904\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.89275 to 0.81996, saving model to nn_model.h5\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.7127 - accuracy: 0.7319 - val_loss: 0.7687 - val_accuracy: 0.7118\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.81996 to 0.76872, saving model to nn_model.h5\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.6286 - accuracy: 0.7730 - val_loss: 0.7302 - val_accuracy: 0.7314\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.76872 to 0.73017, saving model to nn_model.h5\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.5540 - accuracy: 0.8005 - val_loss: 0.7008 - val_accuracy: 0.7446\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.73017 to 0.70075, saving model to nn_model.h5\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4868 - accuracy: 0.8332 - val_loss: 0.6742 - val_accuracy: 0.7564\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.70075 to 0.67422, saving model to nn_model.h5\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.4331 - accuracy: 0.8531 - val_loss: 0.6613 - val_accuracy: 0.7622\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.67422 to 0.66133, saving model to nn_model.h5\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.3904 - accuracy: 0.8654 - val_loss: 0.6572 - val_accuracy: 0.7676\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.66133 to 0.65715, saving model to nn_model.h5\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3536 - accuracy: 0.8808 - val_loss: 0.6583 - val_accuracy: 0.7709\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.65715\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3271 - accuracy: 0.8890 - val_loss: 0.6659 - val_accuracy: 0.7699\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.65715\n",
      "------------------\n",
      "Epoch 1/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.5649 - accuracy: 0.2868 - val_loss: 1.3967 - val_accuracy: 0.4532\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.39670, saving model to nn_model.h5\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 1.2919 - accuracy: 0.4982 - val_loss: 1.0668 - val_accuracy: 0.5873\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.39670 to 1.06679, saving model to nn_model.h5\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.0084 - accuracy: 0.6100 - val_loss: 0.9311 - val_accuracy: 0.6402\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.06679 to 0.93106, saving model to nn_model.h5\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.8600 - accuracy: 0.6741 - val_loss: 0.8547 - val_accuracy: 0.6757\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.93106 to 0.85474, saving model to nn_model.h5\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.7584 - accuracy: 0.7151 - val_loss: 0.8076 - val_accuracy: 0.6931\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85474 to 0.80758, saving model to nn_model.h5\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.6777 - accuracy: 0.7514 - val_loss: 0.7716 - val_accuracy: 0.7102\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.80758 to 0.77164, saving model to nn_model.h5\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.6103 - accuracy: 0.7808 - val_loss: 0.7495 - val_accuracy: 0.7199\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.77164 to 0.74955, saving model to nn_model.h5\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.5484 - accuracy: 0.8045 - val_loss: 0.7338 - val_accuracy: 0.7301\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.74955 to 0.73381, saving model to nn_model.h5\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.5011 - accuracy: 0.8235 - val_loss: 0.7322 - val_accuracy: 0.7332\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.73381 to 0.73221, saving model to nn_model.h5\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.4590 - accuracy: 0.8391 - val_loss: 0.7269 - val_accuracy: 0.7422\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.73221 to 0.72694, saving model to nn_model.h5\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.4236 - accuracy: 0.8553 - val_loss: 0.7330 - val_accuracy: 0.7438\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.72694\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3961 - accuracy: 0.8623 - val_loss: 0.7405 - val_accuracy: 0.7462\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.72694\n",
      "------------------\n",
      "Epoch 1/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.5663 - accuracy: 0.2840 - val_loss: 1.3855 - val_accuracy: 0.4629\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.38546, saving model to nn_model.h5\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.2801 - accuracy: 0.5068 - val_loss: 1.0394 - val_accuracy: 0.6066\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.38546 to 1.03936, saving model to nn_model.h5\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.9771 - accuracy: 0.6259 - val_loss: 0.9012 - val_accuracy: 0.6605\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.03936 to 0.90116, saving model to nn_model.h5\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.8301 - accuracy: 0.6897 - val_loss: 0.8246 - val_accuracy: 0.6924\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.90116 to 0.82455, saving model to nn_model.h5\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.7241 - accuracy: 0.7345 - val_loss: 0.7747 - val_accuracy: 0.7157\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.82455 to 0.77472, saving model to nn_model.h5\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.6379 - accuracy: 0.7709 - val_loss: 0.7373 - val_accuracy: 0.7309\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.77472 to 0.73734, saving model to nn_model.h5\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.5701 - accuracy: 0.7966 - val_loss: 0.7142 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.73734 to 0.71424, saving model to nn_model.h5\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.5114 - accuracy: 0.8205 - val_loss: 0.6975 - val_accuracy: 0.7492\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.71424 to 0.69754, saving model to nn_model.h5\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.4610 - accuracy: 0.8403 - val_loss: 0.6873 - val_accuracy: 0.7546\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.69754 to 0.68731, saving model to nn_model.h5\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.4189 - accuracy: 0.8546 - val_loss: 0.6844 - val_accuracy: 0.7577\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.68731 to 0.68440, saving model to nn_model.h5\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.3815 - accuracy: 0.8688 - val_loss: 0.6859 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.68440\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.3528 - accuracy: 0.8800 - val_loss: 0.6877 - val_accuracy: 0.7627\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.68440\n",
      "------------------\n",
      "Epoch 1/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.5679 - accuracy: 0.2807 - val_loss: 1.4473 - val_accuracy: 0.4252\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.44732, saving model to nn_model.h5\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.3602 - accuracy: 0.4518 - val_loss: 1.1205 - val_accuracy: 0.5664\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.44732 to 1.12054, saving model to nn_model.h5\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 1.0589 - accuracy: 0.5880 - val_loss: 0.9482 - val_accuracy: 0.6381\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.12054 to 0.94821, saving model to nn_model.h5\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.8832 - accuracy: 0.6613 - val_loss: 0.8594 - val_accuracy: 0.6776\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.94821 to 0.85940, saving model to nn_model.h5\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.7696 - accuracy: 0.7131 - val_loss: 0.7993 - val_accuracy: 0.7056\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85940 to 0.79931, saving model to nn_model.h5\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.6761 - accuracy: 0.7520 - val_loss: 0.7532 - val_accuracy: 0.7260\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.79931 to 0.75320, saving model to nn_model.h5\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.6021 - accuracy: 0.7845 - val_loss: 0.7174 - val_accuracy: 0.7420\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.75320 to 0.71740, saving model to nn_model.h5\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.5301 - accuracy: 0.8127 - val_loss: 0.6922 - val_accuracy: 0.7525\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.71740 to 0.69218, saving model to nn_model.h5\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4732 - accuracy: 0.8356 - val_loss: 0.6730 - val_accuracy: 0.7608\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.69218 to 0.67296, saving model to nn_model.h5\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4261 - accuracy: 0.8526 - val_loss: 0.6634 - val_accuracy: 0.7658\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.67296 to 0.66342, saving model to nn_model.h5\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.3846 - accuracy: 0.8680 - val_loss: 0.6598 - val_accuracy: 0.7689\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.66342 to 0.65981, saving model to nn_model.h5\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 3s 6ms/step - loss: 0.3523 - accuracy: 0.8806 - val_loss: 0.6625 - val_accuracy: 0.7718\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.65981\n",
      "Epoch 13/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.3253 - accuracy: 0.8907 - val_loss: 0.6680 - val_accuracy: 0.7737\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.65981\n",
      "------------------\n",
      "Epoch 1/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.5639 - accuracy: 0.2835 - val_loss: 1.3848 - val_accuracy: 0.4839\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.38484, saving model to nn_model.h5\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 1.2792 - accuracy: 0.5182 - val_loss: 1.0536 - val_accuracy: 0.5986\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.38484 to 1.05356, saving model to nn_model.h5\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.9960 - accuracy: 0.6168 - val_loss: 0.9220 - val_accuracy: 0.6456\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.05356 to 0.92197, saving model to nn_model.h5\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.8489 - accuracy: 0.6741 - val_loss: 0.8493 - val_accuracy: 0.6782\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.92197 to 0.84927, saving model to nn_model.h5\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.7483 - accuracy: 0.7205 - val_loss: 0.8001 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.84927 to 0.80012, saving model to nn_model.h5\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.6694 - accuracy: 0.7593 - val_loss: 0.7651 - val_accuracy: 0.7142\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.80012 to 0.76513, saving model to nn_model.h5\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.5983 - accuracy: 0.7866 - val_loss: 0.7375 - val_accuracy: 0.7288\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.76513 to 0.73751, saving model to nn_model.h5\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.5422 - accuracy: 0.8123 - val_loss: 0.7200 - val_accuracy: 0.7385\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.73751 to 0.71996, saving model to nn_model.h5\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4889 - accuracy: 0.8309 - val_loss: 0.7063 - val_accuracy: 0.7461\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.71996 to 0.70629, saving model to nn_model.h5\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4440 - accuracy: 0.8492 - val_loss: 0.6981 - val_accuracy: 0.7516\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.70629 to 0.69811, saving model to nn_model.h5\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4058 - accuracy: 0.8636 - val_loss: 0.6955 - val_accuracy: 0.7562\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.69811 to 0.69550, saving model to nn_model.h5\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3745 - accuracy: 0.8741 - val_loss: 0.6949 - val_accuracy: 0.7595\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.69550 to 0.69495, saving model to nn_model.h5\n",
      "Epoch 13/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3447 - accuracy: 0.8835 - val_loss: 0.7009 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.69495\n",
      "Epoch 14/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3226 - accuracy: 0.8917 - val_loss: 0.7055 - val_accuracy: 0.7633\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.69495\n",
      "------------------\n",
      "Epoch 1/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.5663 - accuracy: 0.2843 - val_loss: 1.3689 - val_accuracy: 0.4727\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.36888, saving model to nn_model.h5\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.2650 - accuracy: 0.5076 - val_loss: 1.0542 - val_accuracy: 0.5945\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.36888 to 1.05423, saving model to nn_model.h5\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 3s 6ms/step - loss: 0.9894 - accuracy: 0.6227 - val_loss: 0.9240 - val_accuracy: 0.6479\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.05423 to 0.92402, saving model to nn_model.h5\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.8471 - accuracy: 0.6841 - val_loss: 0.8505 - val_accuracy: 0.6817\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.92402 to 0.85052, saving model to nn_model.h5\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.7433 - accuracy: 0.7267 - val_loss: 0.8003 - val_accuracy: 0.7054\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85052 to 0.80034, saving model to nn_model.h5\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.6572 - accuracy: 0.7650 - val_loss: 0.7592 - val_accuracy: 0.7236\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.80034 to 0.75922, saving model to nn_model.h5\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.5822 - accuracy: 0.7954 - val_loss: 0.7289 - val_accuracy: 0.7390\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.75922 to 0.72893, saving model to nn_model.h5\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.5184 - accuracy: 0.8197 - val_loss: 0.7093 - val_accuracy: 0.7448\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.72893 to 0.70926, saving model to nn_model.h5\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 2s 4ms/step - loss: 0.4630 - accuracy: 0.8426 - val_loss: 0.6952 - val_accuracy: 0.7534\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.70926 to 0.69517, saving model to nn_model.h5\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 3s 6ms/step - loss: 0.4157 - accuracy: 0.8599 - val_loss: 0.6862 - val_accuracy: 0.7600\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.69517 to 0.68623, saving model to nn_model.h5\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3796 - accuracy: 0.8712 - val_loss: 0.6845 - val_accuracy: 0.7642\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.68623 to 0.68447, saving model to nn_model.h5\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.3485 - accuracy: 0.8823 - val_loss: 0.6873 - val_accuracy: 0.7663\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.68447\n",
      "Epoch 13/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3194 - accuracy: 0.8942 - val_loss: 0.6918 - val_accuracy: 0.7662\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.68447\n",
      "------------------\n",
      "Epoch 1/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.5667 - accuracy: 0.2779 - val_loss: 1.4267 - val_accuracy: 0.4208\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.42671, saving model to nn_model.h5\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.3316 - accuracy: 0.4782 - val_loss: 1.1132 - val_accuracy: 0.5673\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.42671 to 1.11320, saving model to nn_model.h5\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.0479 - accuracy: 0.5959 - val_loss: 0.9438 - val_accuracy: 0.6359\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.11320 to 0.94376, saving model to nn_model.h5\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.8738 - accuracy: 0.6637 - val_loss: 0.8593 - val_accuracy: 0.6777\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.94376 to 0.85929, saving model to nn_model.h5\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.7655 - accuracy: 0.7160 - val_loss: 0.8064 - val_accuracy: 0.7006\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.85929 to 0.80644, saving model to nn_model.h5\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.6782 - accuracy: 0.7573 - val_loss: 0.7672 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.80644 to 0.76716, saving model to nn_model.h5\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.6039 - accuracy: 0.7839 - val_loss: 0.7365 - val_accuracy: 0.7321\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.76716 to 0.73648, saving model to nn_model.h5\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.5395 - accuracy: 0.8117 - val_loss: 0.7175 - val_accuracy: 0.7399\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.73648 to 0.71752, saving model to nn_model.h5\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4850 - accuracy: 0.8326 - val_loss: 0.7068 - val_accuracy: 0.7444\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.71752 to 0.70678, saving model to nn_model.h5\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.4388 - accuracy: 0.8516 - val_loss: 0.7012 - val_accuracy: 0.7519\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.70678 to 0.70119, saving model to nn_model.h5\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.4019 - accuracy: 0.8616 - val_loss: 0.7017 - val_accuracy: 0.7545\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.70119\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3701 - accuracy: 0.8751 - val_loss: 0.7036 - val_accuracy: 0.7557\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.70119\n",
      "------------------\n",
      "Epoch 1/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.5676 - accuracy: 0.2790 - val_loss: 1.4379 - val_accuracy: 0.4472\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.43790, saving model to nn_model.h5\n",
      "Epoch 2/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 1.3244 - accuracy: 0.5032 - val_loss: 1.0451 - val_accuracy: 0.6107\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.43790 to 1.04507, saving model to nn_model.h5\n",
      "Epoch 3/20\n",
      "541/541 [==============================] - 2s 5ms/step - loss: 0.9825 - accuracy: 0.6327 - val_loss: 0.8921 - val_accuracy: 0.6663\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.04507 to 0.89206, saving model to nn_model.h5\n",
      "Epoch 4/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.8214 - accuracy: 0.6974 - val_loss: 0.8024 - val_accuracy: 0.7062\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.89206 to 0.80242, saving model to nn_model.h5\n",
      "Epoch 5/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.7055 - accuracy: 0.7465 - val_loss: 0.7364 - val_accuracy: 0.7355\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.80242 to 0.73641, saving model to nn_model.h5\n",
      "Epoch 6/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.6068 - accuracy: 0.7873 - val_loss: 0.6898 - val_accuracy: 0.7523\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.73641 to 0.68977, saving model to nn_model.h5\n",
      "Epoch 7/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.5320 - accuracy: 0.8138 - val_loss: 0.6600 - val_accuracy: 0.7651\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.68977 to 0.65997, saving model to nn_model.h5\n",
      "Epoch 8/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4711 - accuracy: 0.8371 - val_loss: 0.6455 - val_accuracy: 0.7703\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.65997 to 0.64545, saving model to nn_model.h5\n",
      "Epoch 9/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.4233 - accuracy: 0.8555 - val_loss: 0.6325 - val_accuracy: 0.7774\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.64545 to 0.63252, saving model to nn_model.h5\n",
      "Epoch 10/20\n",
      "541/541 [==============================] - 3s 5ms/step - loss: 0.3829 - accuracy: 0.8678 - val_loss: 0.6292 - val_accuracy: 0.7803\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.63252 to 0.62916, saving model to nn_model.h5\n",
      "Epoch 11/20\n",
      "541/541 [==============================] - 4s 7ms/step - loss: 0.3479 - accuracy: 0.8823 - val_loss: 0.6312 - val_accuracy: 0.7811\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.62916\n",
      "Epoch 12/20\n",
      "541/541 [==============================] - 3s 6ms/step - loss: 0.3190 - accuracy: 0.8916 - val_loss: 0.6369 - val_accuracy: 0.7816\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.62916\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_nn_feats(rnd=1):\n",
    "    train_pred, test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((54879,5)),np.zeros((19617,5))\n",
    "    FEAT_CNT = 10\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 100\n",
    "    NUM_CLASSES = 5\n",
    "    MODEL_P = 'nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    skf = StratifiedKFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in skf.split(ttrain_x,tmp_Y):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        mc = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        es=EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.3,\n",
    "                  batch_size=64, epochs=20, \n",
    "                  verbose=1,\n",
    "                  callbacks=[mc,es],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # feature 생성 1\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # feature 생성 2\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "nn_train1,nn_test1,nn_train2,nn_test2 = get_nn_feats(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nn_train = np.hstack([gru_train1, gru_train2, \n",
    "                        cnn_train1, cnn_train2,cnn_train3, cnn_train4,cnn_train5, cnn_train6,\n",
    "                        nn_train1,nn_train2\n",
    "                        ])\n",
    "all_nn_test = np.hstack([gru_test1, gru_test2, \n",
    "                        cnn_test1, cnn_test2,cnn_test3, cnn_test4,cnn_test5, cnn_test6,\n",
    "                        nn_test1,nn_test2 \n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 303) (19617, 303)\n"
     ]
    }
   ],
   "source": [
    "# 최종 앙상블 데이터\n",
    "cols_to_drop = ['index', 'text','tag_txt','ne_txt']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "train_X = np.hstack([train_X,train_svd,train_svd2,train_cvec3,train_cvec4,train_tf5,train_tf6])\n",
    "test_X = np.hstack([test_X,test_svd,test_svd2,test_cvec3,test_cvec4,test_tf5,test_tf6])\n",
    "\n",
    "f_train_X = np.hstack([train_X, help_train_feat,help_train_feat2,help_train_feat3,all_nn_train])\n",
    "f_train_X = np.round(f_train_X,4) #소수점 4자리 반올림\n",
    "f_test_X = np.hstack([test_X, help_test_feat,help_test_feat2,help_test_feat3,all_nn_test])\n",
    "f_test_X = np.round(f_test_X,4)\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 앙상블입니다.\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 42\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    best_loss = 100\n",
    "    best_single_pred = None\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7, #70%의 column subsampling\n",
    "                'subsample': 0.8, #80%의 데이터 subsampling\n",
    "                'eta': 0.04, #overfitting 방지를 위한 shrinkage 설정\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':5,\n",
    "                'tree_method':'gpu_hist'\n",
    "        }\n",
    "        \n",
    "        d_train = xgb.DMatrix(X_train, y_train) #DMatrix : 넘파이 입력 파라미터를 받아서 만들어지는 XGBoost만의 전용 데이터 세트\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=200)\n",
    "        \n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score\n",
    "        reverse_score += rev_valid_score\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "            best_loss = valid_score\n",
    "            best_single_pred = test_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(d_test)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score #valid_score 대비 curr_pred비 \n",
    "            org_train_pred += tmp_train_pred\n",
    "\n",
    "            if valid_score < best_loss:\n",
    "                print('BETTER')\n",
    "                best_loss = valid_score\n",
    "                best_single_pred = curr_pred\n",
    "\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"sample_submission.csv\")\n",
    "    submiss['0']=test_pred[:,0]\n",
    "    submiss['1']=test_pred[:,1]\n",
    "    submiss['2']=test_pred[:,2]\n",
    "    submiss['3']=test_pred[:,3]\n",
    "    submiss['4']=test_pred[:,4]\n",
    "    submiss.to_csv(\"xgb_{}.csv\".format(k_cnt),index=False)\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['0']=weighted_test_pred[:,0]\n",
    "    submiss['1']=weighted_test_pred[:,1]\n",
    "    submiss['2']=weighted_test_pred[:,2]\n",
    "    submiss['3']=weighted_test_pred[:,3]\n",
    "    submiss['4']=weighted_test_pred[:,4]\n",
    "    submiss.to_csv(\"weighted_{}.csv\".format(k_cnt),index=False)\n",
    "    # best single\n",
    "    submiss=pd.read_csv(\"sample_submission.csv\")\n",
    "    weighted_test_pred = np.round(best_single_pred,4)\n",
    "    submiss['0']=weighted_test_pred[:,0]\n",
    "    submiss['1']=weighted_test_pred[:,1]\n",
    "    submiss['2']=weighted_test_pred[:,2]\n",
    "    submiss['3']=weighted_test_pred[:,3]\n",
    "    submiss['4']=weighted_test_pred[:,4]\n",
    "    submiss.to_csv(\"single_{}.csv\".format(k_cnt),index=False)\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.53928\tvalid-mlogloss:1.53972\n",
      "[200]\ttrain-mlogloss:0.35448\tvalid-mlogloss:0.38397\n",
      "[400]\ttrain-mlogloss:0.31243\tvalid-mlogloss:0.36331\n",
      "[600]\ttrain-mlogloss:0.28499\tvalid-mlogloss:0.35541\n",
      "[800]\ttrain-mlogloss:0.26281\tvalid-mlogloss:0.35129\n",
      "[1000]\ttrain-mlogloss:0.24334\tvalid-mlogloss:0.34838\n",
      "[1200]\ttrain-mlogloss:0.22585\tvalid-mlogloss:0.34702\n",
      "[1400]\ttrain-mlogloss:0.20971\tvalid-mlogloss:0.34591\n",
      "[1600]\ttrain-mlogloss:0.19510\tvalid-mlogloss:0.34499\n",
      "[1611]\ttrain-mlogloss:0.19430\tvalid-mlogloss:0.34493\n",
      "train log loss 0.19430172087236847 valid log loss 0.3449299331269923\n",
      "rev 2.8991395177983335\n",
      "[0]\ttrain-mlogloss:1.53948\tvalid-mlogloss:1.53983\n"
     ]
    }
   ],
   "source": [
    "cv_test(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
